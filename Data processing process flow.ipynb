{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d1e8a7-10c6-4f76-9d47-3e5ac4bf808e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. PATH CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24f7d6e8-406e-4e44-857b-9888f48d0b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Working Directory: C:\\Users\\iurie\\3. Tableau Projects\\02. Security Operation Center Analyst\\2. Data_Processing_Process\n",
      "üìÇ Raw Data Directory detected at: C:\\Users\\iurie\\3. Tableau Projects\\02. Security Operation Center Analyst\\1. Raw_Data_Set\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. PATH CONFIGURATION ---\n",
    "\n",
    "# Current working directory (where this Notebook is located)\n",
    "CURRENT_DIR = os.getcwd()\n",
    "\n",
    "# Move one level up (..) to access the Raw_Data_Set folder\n",
    "RAW_DATA_DIR = os.path.join(CURRENT_DIR, '..', '1. Raw_Data_Set')\n",
    "\n",
    "# Define exact paths for data sources\n",
    "PATH_LINKEDIN = os.path.join(RAW_DATA_DIR, 'LinkedIn')\n",
    "PATH_GLASSDOOR = os.path.join(RAW_DATA_DIR, 'GlassDoor')\n",
    "\n",
    "# Configuration files (located in the same folder as this Notebook)\n",
    "FILE_ROLES = 'SOC_Analyst_Roles.json'\n",
    "FILE_SKILLS = 'Skills.json'\n",
    "FILE_LOCATIONS_FIX = 'Locations.json'\n",
    "\n",
    "# Final output file for Tableau\n",
    "FINAL_OUTPUT = 'SOC_Analyst_Master_Dataset.json'\n",
    "\n",
    "print(f\"üìÇ Working Directory: {CURRENT_DIR}\")\n",
    "print(f\"üìÇ Raw Data Directory detected at: {os.path.abspath(RAW_DATA_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b486da3-5af1-4c1b-bae2-22c83477baec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. LOAD CONFIGURATION FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47cc9a95-284e-4dac-bd40-f87fb45b4faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration files (Roles, Skills, Locations) loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. LOAD CONFIGURATION FILES ---\n",
    "\n",
    "try:\n",
    "    # Load Roles Definition\n",
    "    with open(FILE_ROLES, 'r', encoding='utf-8') as f:\n",
    "        roles_data = json.load(f)\n",
    "        # Handle cases where data is nested under \"all roles SOC\"\n",
    "        ROLES_DEF = roles_data.get(\"all roles SOC\", roles_data)\n",
    "\n",
    "    # Load Skills Definition\n",
    "    with open(FILE_SKILLS, 'r', encoding='utf-8') as f:\n",
    "        skills_data = json.load(f)\n",
    "        SKILLS_DEF = skills_data.get(\"Skills\", skills_data)\n",
    "\n",
    "    # Load Location Fixes (Mapping)\n",
    "    with open(FILE_LOCATIONS_FIX, 'r', encoding='utf-8') as f:\n",
    "        loc_list = json.load(f)\n",
    "        # Convert list to dictionary for O(1) lookup speed using 'location' as key\n",
    "        LOC_FIX_MAP = {item['location']: item for item in loc_list}\n",
    "\n",
    "    print(\"‚úÖ Configuration files (Roles, Skills, Locations) loaded successfully.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå ERROR: Configuration file not found: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå UNEXPECTED ERROR while reading config: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6643c-a902-4bbd-a2e7-1796452a84d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. DATA NORMALIZATION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "effe20f2-69e7-42f4-8f02-4439dfbe2582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. DATA NORMALIZATION FUNCTION ---\n",
    "\n",
    "def normalize_job_structure(job_data, source_name):\n",
    "    \"\"\"\n",
    "    Accepts a raw job object (dict) from either LinkedIn or Glassdoor\n",
    "    and returns a standardized dictionary with unified keys.\n",
    "    \"\"\"\n",
    "    normalized = {}\n",
    "    \n",
    "    if source_name == \"LinkedIn\":\n",
    "        normalized = {\n",
    "            \"title\": job_data.get(\"title\"),\n",
    "            \"companyName\": job_data.get(\"companyName\"),\n",
    "            \"location\": job_data.get(\"location\"),\n",
    "            \"jobUrl\": job_data.get(\"jobUrl\"),\n",
    "            \"contractType\": job_data.get(\"contractType\"),\n",
    "            \"description\": job_data.get(\"description\"), # Sometimes 'descriptionHtml'\n",
    "            \"source\": \"LinkedIn\"\n",
    "        }\n",
    "    elif source_name == \"GlassDoor\":\n",
    "        # Glassdoor location can be a dictionary or a string\n",
    "        loc_obj = job_data.get(\"job_location\")\n",
    "        loc_str = \"\"\n",
    "        if isinstance(loc_obj, dict):\n",
    "            parts = [p for p in [loc_obj.get(\"city\"), loc_obj.get(\"country\")] if p]\n",
    "            loc_str = \", \".join(parts)\n",
    "        elif isinstance(loc_obj, str):\n",
    "            loc_str = loc_obj\n",
    "\n",
    "        normalized = {\n",
    "            \"title\": job_data.get(\"job_title\"),\n",
    "            \"companyName\": job_data.get(\"company_name\"),\n",
    "            \"location\": loc_str,\n",
    "            \"jobUrl\": job_data.get(\"job_url\"),\n",
    "            \"contractType\": job_data.get(\"job_job_types\"),\n",
    "            \"description\": job_data.get(\"job_description\"),\n",
    "            \"source\": \"GlassDoor\"\n",
    "        }\n",
    "    \n",
    "    # Basic string cleaning for location\n",
    "    if normalized['location']:\n",
    "        normalized['location'] = normalized['location'].strip()\n",
    "        \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff0434-3b0e-41a0-9085-57d1f1b90dfd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. DATA PROCESSING PIPELINE (ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62759c7c-9904-43a2-a2bf-1bc1485e7c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Data Processing Pipeline...\n",
      "üìÇ Found 42 JSON files in total.\n",
      "üìä Total raw jobs extracted: 4096\n",
      "\n",
      "--- FINAL EXECUTION REPORT ---\n",
      "1. Total Jobs Scanned: 4096\n",
      "2. Excluded (Irrelevant): -1692\n",
      "3. Excluded (Duplicates): -61\n",
      "==========================================\n",
      "‚úÖ FINAL DATASET FOR TABLEAU: 2343 jobs\n"
     ]
    }
   ],
   "source": [
    "# --- 4. DATA PROCESSING PIPELINE (ETL) ---\n",
    "\n",
    "all_jobs = []\n",
    "processed_jobs = []\n",
    "seen_ids = set() \n",
    "\n",
    "# Statistics for debugging and quality assurance\n",
    "stats = {\n",
    "    \"total_read\": 0,\n",
    "    \"excluded_filter\": 0,\n",
    "    \"excluded_duplicate\": 0,\n",
    "    \"final_count\": 0\n",
    "}\n",
    "\n",
    "# Extended keyword list for initial relevance filtering\n",
    "KEYWORDS_FILTER = [\"soc\", \"security operation\", \"incident response\", \"threat intelligence\", \"cyber defense\", \"siem\", \"blue team\"]\n",
    "\n",
    "print(\"üöÄ Starting Data Processing Pipeline...\")\n",
    "\n",
    "# --- STEP A: INGESTION ---\n",
    "files_list = []\n",
    "files_list.extend(glob.glob(os.path.join(PATH_LINKEDIN, \"*.json\")))\n",
    "files_list.extend(glob.glob(os.path.join(PATH_GLASSDOOR, \"*.json\")))\n",
    "\n",
    "print(f\"üìÇ Found {len(files_list)} JSON files in total.\")\n",
    "\n",
    "for file in files_list:\n",
    "    try:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            source = \"LinkedIn\" if \"LinkedIn\" in file else \"GlassDoor\"\n",
    "            \n",
    "            # Handle both List and Dict structures in JSON\n",
    "            if isinstance(data, list):\n",
    "                for job in data:\n",
    "                    all_jobs.append(normalize_job_structure(job, source))\n",
    "            elif isinstance(data, dict):\n",
    "                 all_jobs.append(normalize_job_structure(data, source))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading file {os.path.basename(file)}: {e}\")\n",
    "\n",
    "stats[\"total_read\"] = len(all_jobs)\n",
    "print(f\"üìä Total raw jobs extracted: {stats['total_read']}\")\n",
    "\n",
    "\n",
    "# --- STEP B: TRANSFORMATION (Filtering, Deduplication, Enrichment) ---\n",
    "\n",
    "for job in all_jobs:\n",
    "    title = str(job.get(\"title\", \"\")).lower()\n",
    "    desc = str(job.get(\"description\", \"\")).lower()\n",
    "    \n",
    "    # 1. RELEVANCE FILTERING: Check Title OR Description\n",
    "    is_relevant = any(kw in title for kw in KEYWORDS_FILTER) or any(kw in desc for kw in KEYWORDS_FILTER)\n",
    "    \n",
    "    if not is_relevant:\n",
    "        stats[\"excluded_filter\"] += 1\n",
    "        continue \n",
    "\n",
    "    # 2. DEDUPLICATION: Create a composite key (Title + Company + Location)\n",
    "    # Removing spaces and commas to handle slight variations in formatting\n",
    "    comp = str(job.get(\"companyName\", \"\")).lower().replace(\" \", \"\").replace(\",\", \"\")\n",
    "    loc = str(job.get(\"location\", \"\")).lower().replace(\" \", \"\").replace(\",\", \"\")\n",
    "    tit_simple = title.replace(\" \", \"\")\n",
    "    \n",
    "    unique_id = f\"{tit_simple}|{comp}|{loc}\"\n",
    "    \n",
    "    if unique_id in seen_ids:\n",
    "        stats[\"excluded_duplicate\"] += 1\n",
    "        continue\n",
    "    seen_ids.add(unique_id)\n",
    "\n",
    "    # 3. GEOGRAPHIC PARSING (Split Location into City/Region/Country)\n",
    "    job['city'] = None\n",
    "    job['region'] = None\n",
    "    job['country'] = None\n",
    "    \n",
    "    raw_loc = job.get(\"location\", \"\")\n",
    "    if raw_loc:\n",
    "        parts = [p.strip() for p in raw_loc.split(',')]\n",
    "        if len(parts) >= 3:\n",
    "            job['city'] = parts[0]\n",
    "            job['region'] = \", \".join(parts[1:-1])\n",
    "            job['country'] = parts[-1]\n",
    "        elif len(parts) == 2:\n",
    "            job['city'] = parts[0]\n",
    "            job['country'] = parts[1]\n",
    "        elif len(parts) == 1:\n",
    "            job['country'] = parts[0]\n",
    "            \n",
    "    # 4. LOCATION FIXES (Apply manual corrections from Locations.json)\n",
    "    if job.get('region') is None and job.get('location') in LOC_FIX_MAP:\n",
    "        fix_data = LOC_FIX_MAP[job['location']]\n",
    "        job['country'] = fix_data.get('country')\n",
    "        job['region'] = fix_data.get('region')\n",
    "        job['city'] = fix_data.get('city')\n",
    "\n",
    "    # 5. ROLE ASSIGNMENT (Using Rule-based Classification)\n",
    "    assigned_role = \"Uncategorized\"\n",
    "    \n",
    "    # Extended dictionary to catch specific roles not in the main JSON (Includes German terms & Niche roles)\n",
    "    extended_roles = {\n",
    "        \"Direct SOC Analyst\": [\n",
    "            \"soc\", \"security analyst\", \"analyst\", \"operator\", \"monitoring\", \n",
    "            \"intern\", \"student\", \"trainee\", \"junior\", \"graduate\", \"stage\", \n",
    "            \"werkstudent\", \"watch\", \"handler\", \"analyste\", \n",
    "            \"duales\", \"placement\", \"apprentice\"\n",
    "        ],\n",
    "        \"SOC Technology Specialists\": [\n",
    "            \"engineer\", \"network\", \"system\", \"admin\", \"integrator\", \"architect\", \n",
    "            \"support\", \"technician\", \"embedded\", \"infrastructure\", \"developer\",\n",
    "            \"iam\", \"pki\", \"access\", \"identity\", \"cloud\", \"devops\", \"platform\",\n",
    "            \"pentest\", \"hacker\", \"offensive\", \"vulnerability\", \"tester\",\n",
    "            \"spezialist\", \"specialist\", \"expert\", \"sicherheit\", \"security professional\",\n",
    "            \"engineer\", \"ingenieur\", \"product owner\", \"scientist\"\n",
    "        ],\n",
    "        \"Incident Response and Threat Intelligence\": [\n",
    "            \"incident\", \"response\", \"threat\", \"intelligence\", \"forensic\", \n",
    "            \"hunter\", \"cert\", \"csirt\", \"ir\", \"malware\", \n",
    "            \"investigator\", \"crypto\"\n",
    "        ],\n",
    "        \"Sales, Management, Training & Consulting\": [\n",
    "            \"manager\", \"head\", \"lead\", \"sales\", \"consultant\", \"trainer\", \n",
    "            \"director\", \"coordinator\", \"officer\", \"ciso\", \"presales\", \"account\",\n",
    "            \"risk\", \"compliance\", \"audit\", \"governance\", \"awareness\", \"legal\",\n",
    "            \"advisor\", \"leiter\", \"vp\", \"vice president\", \"president\",\n",
    "            \"professor\", \"lecturer\", \"teacher\", \"academic\", \"research\",\n",
    "            \"advocate\", \"claims\"\n",
    "        ],\n",
    "        \"Cyber Defense and Operations\": [\n",
    "            \"defense\", \"defence\", \"blue\", \"operations\", \"ops\", \"secops\", \"protect\", \"resilience\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # A. First pass: Check against the main JSON config file\n",
    "    match_found = False\n",
    "    for role_name, keywords in ROLES_DEF.items():\n",
    "        if any(k.lower() in title for k in keywords):\n",
    "            assigned_role = role_name\n",
    "            match_found = True\n",
    "            break\n",
    "            \n",
    "    # B. Second pass: Check against the extended hardcoded list\n",
    "    if not match_found:\n",
    "        for role_name, keywords in extended_roles.items():\n",
    "            if any(k in title for k in keywords):\n",
    "                assigned_role = role_name\n",
    "                break\n",
    "                \n",
    "    job['role'] = assigned_role\n",
    "\n",
    "    # 6. SKILLS EXTRACTION (Keyword matching in Description)\n",
    "    job_skills = {}\n",
    "    for category, skill_list in SKILLS_DEF.items():\n",
    "        found_skills = []\n",
    "        for skill in skill_list:\n",
    "            # Regex \\b ensures whole word matching (e.g., avoids matching \"Java\" in \"Javascript\")\n",
    "            if re.search(r'\\b' + re.escape(skill) + r'\\b', desc, re.IGNORECASE):\n",
    "                found_skills.append(skill)\n",
    "        if found_skills:\n",
    "            job_skills[category] = found_skills\n",
    "            \n",
    "    job['skills'] = job_skills\n",
    "\n",
    "    processed_jobs.append(job)\n",
    "\n",
    "stats[\"final_count\"] = len(processed_jobs)\n",
    "\n",
    "print(\"\\n--- FINAL EXECUTION REPORT ---\")\n",
    "print(f\"1. Total Jobs Scanned: {stats['total_read']}\")\n",
    "print(f\"2. Excluded (Irrelevant): -{stats['excluded_filter']}\")\n",
    "print(f\"3. Excluded (Duplicates): -{stats['excluded_duplicate']}\")\n",
    "print(f\"==========================================\")\n",
    "print(f\"‚úÖ FINAL DATASET FOR TABLEAU: {stats['final_count']} jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d53d07-7d49-4659-b4ca-6b3500164a9a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5. CHECKING DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0a28dd4-df26-4187-b779-1abe771b8476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting Data Quality Check...\n",
      "\n",
      "--- üìä Job Role Distribution ---\n",
      "role\n",
      "SOC Technology Specialists                   1184\n",
      "Direct SOC Analyst                            746\n",
      "Sales, Management, Training & Consulting      201\n",
      "Uncategorized                                 116\n",
      "Incident Response and Threat Intelligence      76\n",
      "Cyber Defense and Operations                   20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- üé≤ Random Sample of 5 Jobs (Clean Data Preview) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>companyName</th>\n",
       "      <th>location</th>\n",
       "      <th>contractType</th>\n",
       "      <th>source</th>\n",
       "      <th>city</th>\n",
       "      <th>region</th>\n",
       "      <th>country</th>\n",
       "      <th>role</th>\n",
       "      <th>skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>SOC Analyst Tier 2 (m/w/d) in Leipzig</td>\n",
       "      <td>WBS IT-Service</td>\n",
       "      <td>leipzig, germany</td>\n",
       "      <td>[]</td>\n",
       "      <td>GlassDoor</td>\n",
       "      <td>leipzig</td>\n",
       "      <td>None</td>\n",
       "      <td>germany</td>\n",
       "      <td>Direct SOC Analyst</td>\n",
       "      <td>{'Programming languages': ['Python'], 'Ability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>Cybersecurity enthusiast</td>\n",
       "      <td>Thales Cyber Solutions Luxembourg</td>\n",
       "      <td>Contern, Luxembourg, Luxembourg</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>Contern</td>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>Luxembourg</td>\n",
       "      <td>Direct SOC Analyst</td>\n",
       "      <td>{'Ability': ['Cybersecurity', 'English', 'Fren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>Active Directory Expert</td>\n",
       "      <td>ENGIE</td>\n",
       "      <td>Bucharest, Bucharest, Romania</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>Bucharest</td>\n",
       "      <td>Bucharest</td>\n",
       "      <td>Romania</td>\n",
       "      <td>SOC Technology Specialists</td>\n",
       "      <td>{'Programming languages': ['PowerShell'], 'Abi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Sicherheitsexpert:in</td>\n",
       "      <td>pester pac automation GmbH</td>\n",
       "      <td>Wolfertschwenden, Bavaria, Germany</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>Wolfertschwenden</td>\n",
       "      <td>Bavaria</td>\n",
       "      <td>Germany</td>\n",
       "      <td>SOC Technology Specialists</td>\n",
       "      <td>{'Ability': ['Cybersecurity', 'Software develo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>Product Cybersecurity Expert</td>\n",
       "      <td>Roche</td>\n",
       "      <td>Sant Cugat del Vall√®s, Catalonia, Spain</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>Sant Cugat del Vall√®s</td>\n",
       "      <td>Catalonia</td>\n",
       "      <td>Spain</td>\n",
       "      <td>SOC Technology Specialists</td>\n",
       "      <td>{'Ability': ['Computer Science', 'Work experie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title                        companyName                                 location contractType     source                   city      region     country                        role                                             skills\n",
       "2010  SOC Analyst Tier 2 (m/w/d) in Leipzig                     WBS IT-Service                         leipzig, germany           []  GlassDoor                leipzig        None     germany          Direct SOC Analyst  {'Programming languages': ['Python'], 'Ability...\n",
       "1933               Cybersecurity enthusiast  Thales Cyber Solutions Luxembourg          Contern, Luxembourg, Luxembourg    Full-time   LinkedIn                Contern  Luxembourg  Luxembourg          Direct SOC Analyst  {'Ability': ['Cybersecurity', 'English', 'Fren...\n",
       "1769                Active Directory Expert                              ENGIE            Bucharest, Bucharest, Romania    Full-time   LinkedIn              Bucharest   Bucharest     Romania  SOC Technology Specialists  {'Programming languages': ['PowerShell'], 'Abi...\n",
       "202                    Sicherheitsexpert:in         pester pac automation GmbH       Wolfertschwenden, Bavaria, Germany    Full-time   LinkedIn       Wolfertschwenden     Bavaria     Germany  SOC Technology Specialists  {'Ability': ['Cybersecurity', 'Software develo...\n",
       "1655           Product Cybersecurity Expert                              Roche  Sant Cugat del Vall√®s, Catalonia, Spain    Full-time   LinkedIn  Sant Cugat del Vall√®s   Catalonia       Spain  SOC Technology Specialists  {'Ability': ['Computer Science', 'Work experie..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data Check Complete. Total Records: 2343\n"
     ]
    }
   ],
   "source": [
    "# --- 5. CHECKING DATA SET (Quality Assurance) ---\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display # Import necessary for nice HTML tables\n",
    "\n",
    "print(\"üîç Starting Data Quality Check...\")\n",
    "\n",
    "# Convert the list of processed jobs into a Pandas DataFrame\n",
    "df = pd.DataFrame(processed_jobs)\n",
    "\n",
    "# 1. Analyze Job Role Distribution\n",
    "print(\"\\n--- üìä Job Role Distribution ---\")\n",
    "print(df['role'].value_counts())\n",
    "\n",
    "# 2. Visual Inspection (HTML Table)\n",
    "print(\"\\n--- üé≤ Random Sample of 5 Jobs (Clean Data Preview) ---\")\n",
    "\n",
    "# Setup generic pandas display options to ensure text isn't cut off too aggressively\n",
    "pd.set_option('display.max_colwidth', 50) \n",
    "\n",
    "# Create a clean view by dropping the heavy text columns just for this preview\n",
    "# We keep 'jobUrl' this time but truncate it via display options if needed, \n",
    "# or drop it if it's still too messy. Let's drop description/html for clarity.\n",
    "cols_to_hide = ['description', 'descriptionHtml', 'companyUrl', 'jobUrl']\n",
    "df_display = df.drop(columns=cols_to_hide, errors='ignore')\n",
    "\n",
    "# USE DISPLAY() INSTEAD OF PRINT()\n",
    "# This renders a beautiful HTML table in Jupyter Notebooks\n",
    "display(df_display.sample(5))\n",
    "\n",
    "print(f\"\\n‚úÖ Data Check Complete. Total Records: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9159d42-8829-4d39-91c7-b1ee091157d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 6. FINAL EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b16b0a35-5d7f-4ae3-8ed3-33ff80c1772d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Success! The file 'SOC_Analyst_Master_Dataset.json' has been generated.\n",
      "This file is now ready for Tableau ingestion.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. FINAL EXPORT ---\n",
    "\n",
    "try:\n",
    "    with open(FINAL_OUTPUT, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_jobs, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"üéâ Success! The file '{FINAL_OUTPUT}' has been generated.\")\n",
    "    print(\"This file is now ready for Tableau ingestion.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
